{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "4EfrzyQAj18p",
        "colab_type": "code",
        "outputId": "33f5264a-1342-4455-b04d-7bc3db0c589a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "CzMInr1m55eM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "s=tf.Session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "msmMTLAL69zj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Input tensor contains the input sequence and therefore determines the number of time steps\n",
        "shape of x --> [**batch_size, input_sequence, embedding_size**]</br>\n",
        "input_sequence = number of time steps"
      ]
    },
    {
      "metadata": {
        "id": "vWErU91JJ1F0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "at_type='additive'\n",
        "batch_size=32\n",
        "e_input_sequence=10\n",
        "d_input_sequence=6\n",
        "d=50 #embedding size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "by55NZiGmiLE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x=tf.random_uniform(shape=[batch_size, e_input_sequence, d],minval=-1,maxval=1,dtype=tf.float32)\n",
        "y=tf.random_uniform(shape=[batch_size, d_input_sequence, d],minval=-1,maxval=1,dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L-4ekSao3KWf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Understanding keras LSTM() \n",
        "lstm1, state_h, state_c = LSTM(1, return_state=True)\n",
        "</br>**lstm1** ----> hidden state sequence of LSTM at each time-step. &nbsp;&nbsp;if **return_sequence= True** \n",
        "</br>**lstm1** ----> hidden state output of LSTM after all time-steps. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if **return_sequence= False** \n",
        "</br>**state_h**---> hidden state output of LSTM after all time-steps\n",
        "</br>**state_c** ----> cell state of the LSTM cell after the last time-step"
      ]
    },
    {
      "metadata": {
        "id": "wz-3qvjK8NFO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "e_out_dims=x.get_shape().as_list()[2]\n",
        "d_out_dims=y.get_shape().as_list()[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gVTfUI288NMX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "out_dims specifies the dimensionality of the output of the LSTM, i.e dimensionality of the hidden state."
      ]
    },
    {
      "metadata": {
        "id": "b38ciR2R_e4S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Encoder"
      ]
    },
    {
      "metadata": {
        "id": "HUOZOiSAb6xG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f_encoder = tf.keras.layers.CuDNNLSTM(e_out_dims,return_sequences=True,return_state=True)\n",
        "b_encoder = tf.keras.layers.CuDNNLSTM(e_out_dims,return_sequences=True, return_state=True,go_backwards=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UEW8PzDij-ix",
        "colab_type": "code",
        "outputId": "c46f1d37-e8be-4b33-b347-dba918da5906",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "fw_encoder_seq,_, fw_encoder_state = f_encoder(x)\n",
        "bw_encoder_seq,_,bw_encoder_state = b_encoder(x)\n",
        "#return sequences ----> hidden state value at each timestep is returned. Used when stacking LSTMs\n",
        "concat_encoder_seq=tf.concat([fw_encoder_seq,bw_encoder_seq],2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SIy2G1xVhG-Z",
        "colab_type": "code",
        "outputId": "026a03f9-b6c8-4d4b-ba00-85d28ec5512f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "concat_encoder_seq.get_shape().as_list()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[32, 10, 100]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "bU9rXJyfR1Su",
        "colab_type": "code",
        "outputId": "dee5de3a-3913-45fb-f98d-d0e536cffc64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "embed=tf.reshape(concat_encoder_seq,[-1,2*d])\n",
        "squisher = tf.Variable(tf.random_uniform([2*d,d],minval=-1,maxval=1),dtype=tf.float32)\n",
        "encoder_seq=tf.matmul(embed,squisher)\n",
        "encoder_seq=tf.reshape(encoder_seq,[batch_size,e_input_sequence,d])\n",
        "encoder_seq.get_shape().as_list()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[32, 10, 50]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "0rLuj_-gk8Re",
        "colab_type": "code",
        "outputId": "5264356e-6171-4ce2-d089-219b036d203f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "encoder_seq[:,-1,:].get_shape().as_list()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[32, 50]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "1weuPYVu_mnb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Decoder"
      ]
    },
    {
      "metadata": {
        "id": "BXwonw2L_pho",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#####################################################just input one token here. Instead of y\n",
        "decoder=tf.keras.layers.CuDNNLSTM(d_out_dims,return_state=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LfSzrpSRjoC0",
        "colab_type": "code",
        "outputId": "f5772e6f-dff9-493e-ee99-967cbb6b5492",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "cell_type": "code",
      "source": [
        "decoder_seq,decoder_state = decoder(y,initial_state=[encoder_seq[:,-1,:],fw_encoder_state])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-7a5d8aa58d47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdecoder_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoder_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfw_encoder_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ps-36HFjdBGq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Why attention?\n",
        "##What's the shortcoming of seq2seq?\n",
        "So far we've looked at encoder and decoder networks (both are LSTM networks here).\n",
        "Seq2seq transforms input sequence (source) to a target output sequence.\n",
        "In vanilla encoder-decoder models, only the last state of the encoder LSTM is used by the decoder.</br>\n",
        "![alt text](https://lilianweng.github.io/lil-log/assets/images/encoder-decoder-example.png =500x)\n",
        "</br>Consequently seq2seq models tend to forget the source words that came long before the final state. In other words, seq2seq models \"forget\" long source sentences.\n",
        "\n",
        "##How attention mechanism addresses this shortcoming?\n",
        "Qouted from [here](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)\n",
        ">\"In a nutshell, attention in the deep learning can be broadly interpreted as a vector of importance weights: in order to predict or infer one element, such as a pixel in an image or a word in a sentence, we estimate using the attention vector how strongly it is correlated with (or “attends to” as you may have read in many papers) other elements and take the sum of their values weighted by the attention vector as the approximation of the target.\"\n",
        ">\n",
        "![alt text](https://lilianweng.github.io/lil-log/assets/images/sentence-example-attention.png =500x)</br>\n",
        "</br>Instead of only initialising the decoder with final state of the encoder, the context vector aka \"thought\" vector is the average of all encoder hidden states weighted by attention. </br>Note: the attention weights vary for the different decoder time steps.</br>Thus, the entire source input is used at each stage of the decoder wherein, different source words attend to different target words by varying degrees as specified by the attention (alignment) score matrix.</br>The matrix of alignment scores is a nice byproduct to explicitly show the correlation between source and target words.\n",
        "</br>![alt text](https://lilianweng.github.io/lil-log/assets/images/bahdanau-fig3.png =500x)"
      ]
    },
    {
      "metadata": {
        "id": "B_hpnW0zsxFS",
        "colab_type": "code",
        "outputId": "9dad3dd9-c945-4d4a-dd90-cff7474f1408",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "sesh=tf.Session()\n",
        "init=tf.global_variables_initializer()\n",
        "sesh.run(init)\n",
        "print(sesh.run(encoder_seq).shape)\n",
        "print(sesh.run(decoder_seq).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 10, 50)\n",
            "(32, 6, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kZNoedK6RijS",
        "colab_type": "code",
        "outputId": "5be84f19-3154-4d60-990f-c2578d8986b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "s=decoder_seq[:,1,:]\n",
        "s=tf.expand_dims(s,1)\n",
        "multiply=tf.constant([1,e_input_sequence,1])\n",
        "s=tf.tile(s,multiply)\n",
        "e=encoder_seq*s\n",
        "e=tf.reduce_sum(e,axis=2)\n",
        "\n",
        "\n",
        "### replace e with alpha in original code\n",
        "e=tf.reshape(e,[-1,1])\n",
        "temp=tf.reshape(encoder_seq,[-1,e_out_dims])\n",
        "mul=tf.constant([1,e_out_dims])\n",
        "e=tf.tile(e,mul)\n",
        "weighted=e*temp\n",
        "weighted=tf.reshape(weighted,[batch_size,e_input_sequence,e_out_dims])\n",
        "weighted=tf.reduce_sum(weighted,axis=1)\n",
        "weighted.get_shape().as_list()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[32, 50]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "5e5SIeKioSQu",
        "colab_type": "code",
        "outputId": "15a4003a-4ce4-4642-d980-0d6197919604",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "a=tf.constant([['b1_h1'],['b1_h2'],['b2_h1'],['b2_h2']])\n",
        "b=tf.reshape(a,[2,2,1])\n",
        "sesu=tf.Session()\n",
        "sesu.run(a)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[b'b1_h1'],\n",
              "       [b'b1_h2'],\n",
              "       [b'b2_h1'],\n",
              "       [b'b2_h2']], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "metadata": {
        "id": "iXqwt-d4otw3",
        "colab_type": "code",
        "outputId": "a50d676a-0444-4257-dba3-75be27929b84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "sesu.run(b)[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[b'b1_h1'],\n",
              "       [b'b1_h2']], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "metadata": {
        "id": "Ruolm8W-Nkkw",
        "colab_type": "code",
        "outputId": "ac29bf6c-e4bf-40f0-d3aa-c213d5e68b9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(batch_size):\n",
        "  for t in range(d_input_sequence):\n",
        "    H = encoder_seq[i].get_shape().as_list()\n",
        "    s = decoder_seq[i,t] #################edit this after editing decoder network input\n",
        "    ctx = get_context(H,s,at_type)\n",
        "    s,c = tf.keras.layers.CuDNNLSTM(d_out_dims,return_state=True)(next_word,[ctx,decoder_state])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10, 50]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "metadata": {
        "id": "ABd-lyual9_x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FQoCpxs6or-W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Defining the score function"
      ]
    },
    {
      "metadata": {
        "id": "ECPg_L2F2pyB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#function that calulates score based on the type of attention\n",
        "def score(encoder_states,s,att_type):\n",
        "  f=32 #hyperparamter of sorts\n",
        "  n=encoder_states.get_shape().as_list()[1]\n",
        "  m=s.get_shape().as_list()\n",
        "  d=encoder_states.get_shape().as_list()[0]\n",
        "  if att_type=='additive':\n",
        "    W=tf.Variable(tf.random_uniform(shape=[f,2*d],minval=-1,maxval=1))\n",
        "    v=tf.Variable(tf.random_uniform(shape=[1,f],minval=-1,maxval=1))\n",
        "    mat_s=tf.tile(s,[1,n])\n",
        "    hs=tf.stack(encoder_states,mat_s)\n",
        "    temp=tf.nn.tanh(tf.matmul(W,[]))\n",
        "  if att_type=='multiplicative':\n",
        "    #s will be [32,1,50]\n",
        "    s=tf.expand_dims(s,1)\n",
        "    multiply=tf.constant([1,e_input_sequence,1])\n",
        "    alphas=tf.reduce_sum(encoder_seq*s,axis=2)\n",
        "  if att_type=='sdot':\n",
        "    s=tf.expand_dims(s,1)\n",
        "    multiply=tf.constant([1,e_input_sequence,1])\n",
        "    alphas=tf.reduce_sum(encoder_states*s,axis=2)\n",
        "    alphas=1/(e_input_sequence**0.5)\n",
        "    print(\"bitch\")#code goes here\n",
        "  if att_type=='kv':\n",
        "    #code goes here\n",
        "    print(\"fkjden\")\n",
        "    \n",
        "  return alphas # return a [batch_size x n] vector\n",
        "    #returns the softmax vector of dimensions = no. of encoder time steps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tPJVqweiq7YW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#function to construct context vector for given decoder state\n",
        "def get_context(encoder_states,prev_decoder_state,attention):\n",
        "  alpha=score(encoder_states,prev_decoder_state,attention)\n",
        "  alpha=tf.reshape(alpha,[-1,1])\n",
        "  temp=tf.reshape(encoder_states,[-1,e_out_dims])\n",
        "  mul=tf.constant([1,e_out_dims])\n",
        "  alpha=tf.tile(alpha,mul)\n",
        "  ctx=alpha*temp\n",
        "  ctx=tf.reshape(ctx,[batch_size,e_input_sequence,e_out_dims])\n",
        "  ctx=tf.reduce_sum(ctx,axis=1)\n",
        "  \n",
        "  return ctx  #returns context vectors for all batches for this time-step\n",
        "  #if n = number of source words\n",
        "  #encoder_states = [batch_size,n,embedding_size]\n",
        "  #alpha = [bactch_size,n]\n",
        "  #ctx = [batch_size,embedding_size]\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s3DWgVJHowgm",
        "colab_type": "code",
        "outputId": "1f568305-5661-4b4b-ebf5-0926501c599b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "decoder_seq[:,1,:].get_shape().as_list()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[32, 50]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "KgxgGRYYo4xk",
        "colab_type": "code",
        "outputId": "e014beec-20cb-4f8d-d994-e518cf29319e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "weighted.get_shape().as_list()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[32, 50]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "erIEigr7pivW",
        "colab_type": "code",
        "outputId": "c77207d5-46ed-45de-8b2b-8e14c3e276ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "s=tf.Session()\n",
        "a = tf.constant(np.array([[.1, .3, .5, .9],[.1, .3, .5, .9]]))\n",
        "t= tf.constant(np.array([[1, 0, 0, 0],[0, 0, 1, 0]]))  \n",
        "loss_=tf.nn.softmax(a)\n",
        "s.run(loss_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.16838508, 0.205666  , 0.25120102, 0.37474789],\n",
              "       [0.16838508, 0.205666  , 0.25120102, 0.37474789]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "metadata": {
        "id": "XJhWOUbWwxt1",
        "colab_type": "code",
        "outputId": "a8e846a1-3910-4f1d-c33a-8b396c4b84a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#tester\n",
        "np.log(0.16838508)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1.7815017796738182"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "metadata": {
        "id": "KfEfxNVPv8KH",
        "colab_type": "code",
        "outputId": "195365c7-5a8a-48a5-8d5a-0d55fac4344d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "gloss=tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels=t,logits=a),axis=0)\n",
        "print(s.run(gloss))\n",
        "#[[ 0.16838508  0.205666    0.25120102  0.37474789]]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.163003538329237\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Qf7weo5YvsNJ",
        "colab_type": "code",
        "outputId": "31f90070-ba1a-4041-c70a-ae68c5169d13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "s.run(t)\n",
        "t.get_shape().as_list()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 4]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "metadata": {
        "id": "fkCvH8nbpHmz",
        "colab_type": "code",
        "outputId": "44a6785b-b1db-4d6e-ae6c-d135f8df3d71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "t_vocab_size=30000\n",
        "init=tf.global_variables_initializer()\n",
        "s.run(init)\n",
        "r=tf.concat([decoder_seq[:,1,:],weighted],axis=1)\n",
        "d=r.get_shape().as_list()[1]\n",
        "W_loss=tf.Variable(tf.random_uniform([d,t_vocab_size],minval=-1,maxval=1))\n",
        "vec_loss = tf.matmul(r,W_loss)\n",
        "vec_loss = tf.nn.softmax(vec_loss)\n",
        "vec_loss.get_shape().as_list()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[32, 30000]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "metadata": {
        "id": "O96vcbi_oj78",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loss(ctx,s,target_word):   #pass the target word as [batch_size, t_vocab_size] one hot vector\n",
        "  r=tf.concat([s,ctx],axis=1)\n",
        "  d=r.get_shape().as_list()[1]\n",
        "  t_vocab_size=target_word.get_shape().as_list()[0]\n",
        "  W_loss=tf.Variable(tf.random_uniform([d,t_vocab_size],minval=-1,maxval=1))\n",
        "  vec_loss = tf.matmul(r,W_loss)\n",
        "  loss_val = tf.nn.softmax_cross_entropy_with_logits(labels=target_word,logits=vec_loss)\n",
        "   \n",
        "  return loss_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wqGV5mDlkV-Y",
        "colab_type": "code",
        "outputId": "07c6561f-73e1-4bb6-b447-d8814228a432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "decoder_seq.get_shape().as_list()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[32, 6, 50]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "i7Epb8yzprn8",
        "colab_type": "code",
        "outputId": "ee92545b-9531-4d91-ad8e-a88825c7110c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "a=tf.constant([[2],[3]])\n",
        "b=tf.constant([[[2],[2]],[[2],[1]],[[5],[2]]])\n",
        "mat_a=tf.tile(a,[1,10])\n",
        "b.get_shape().as_list()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 2, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "nJBgq7zgGIze",
        "colab_type": "code",
        "outputId": "d562f040-57d2-4c2a-b8ee-35a81d55d5ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "sess=tf.Session()\n",
        "init_=tf.global_variables_initializer()\n",
        "sess.run(init_)\n",
        "sess.run(a)\n",
        "sess.run(b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
              "       [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "cMZGRTIHGsmk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(a=10):\n",
        "  print(a*b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zAiCJUg4PZXL",
        "colab_type": "code",
        "outputId": "0a1899c4-889a-43fd-f3ea-c5c0a4d1812c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "global b\n",
        "b=10\n",
        "test()\n",
        "test(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "50\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}