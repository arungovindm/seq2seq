{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq_wAttention.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "fiS6KTQl7ya8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Pre-processing parameters\n",
        "</br>seq length threshold</br>count threshold\n",
        "\n",
        "\n",
        "##Hyperparamters of the model\n",
        "</br>batch_size\n",
        "</br>embedding dimensions\n",
        "</br>LSTM units"
      ]
    },
    {
      "metadata": {
        "id": "p7Gk35Vv8hYC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Install and import dependencies"
      ]
    },
    {
      "metadata": {
        "id": "U3JDmVKHtYTX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "63c56738-a2b7-4f3f-af76-d697fbda70c5"
      },
      "cell_type": "code",
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.4.1-cp36-cp36m-linux_x86_64.whl \n",
        "!pip3 install torchvision"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==0.4.1 from http://download.pytorch.org/whl/cu80/torch-0.4.1-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.2.post3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (0.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f5qyxGzFtiOu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e316ffcf-260c-4c56-cebb-aa4d9df48774"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "import time\n",
        "import keras\n",
        "import string\n",
        "\n",
        "print(torch.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ISvulUdX8oPh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Load data"
      ]
    },
    {
      "metadata": {
        "id": "tUhh6TpDt3RT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1ba1c940-0a30-47f4-f9d6-e4a8a580b57a"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ElSfuFU_uMnI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "en = open('/content/drive/My Drive/nmt/europarl-v7.de-en.en', encoding='UTF-8').read().strip().split('\\n')\n",
        "de = open('/content/drive/My Drive/nmt/europarl-v7.de-en.de', encoding='UTF-8').read().strip().split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_xmbMSXI80qp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Building vocabulary"
      ]
    },
    {
      "metadata": {
        "id": "UgpRhZ0p8uCz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Set vocabulary Hyper-parameters"
      ]
    },
    {
      "metadata": {
        "id": "0M707824vUTK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sent_threshold=15\n",
        "count_threshold=40"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2uojILuXwBDV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "de_m=[]\n",
        "en_m=[]\n",
        "for i in range(len(de)):\n",
        "  if len(de[i].split(' '))< sent_threshold+1 and len(en[i].split(' '))< sent_threshold+1:\n",
        "    en_m.append(en[i])\n",
        "    de_m.append(de[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xbv-nMpl8-qa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Tokenize"
      ]
    },
    {
      "metadata": {
        "id": "3QD1Ca1CwDtN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "t= Tokenizer(num_words=30000,lower=True,oov_token='<UNK>')\n",
        "t.fit_on_texts(en_m)\n",
        "\n",
        "t_d= Tokenizer(num_words=30000,lower=True,oov_token='<UNK>')\n",
        "t_d.fit_on_texts(de_m)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jStBjTOhxGWN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "t2=Tokenizer()\n",
        "t_d2=Tokenizer()\n",
        "t2.word_counts['<UNK>']=0\n",
        "for word,counts in t.word_counts.items():\n",
        "  if counts < count_threshold:\n",
        "    t2.word_index[word]=3\n",
        "    t2.word_counts['<UNK>']=t2.word_counts['<UNK>']+counts\n",
        "    \n",
        "  else:\n",
        "    t2.word_index[word]=t.word_index[word]+2\n",
        "    t2.word_counts[word]=t.word_counts[word]\n",
        "    \n",
        "t_d2.word_counts['<UNK>']=0\n",
        "for word,counts in t_d.word_counts.items():\n",
        "  if counts < count_threshold:\n",
        "    t_d2.word_index[word]=3\n",
        "    t_d2.word_counts['<UNK>']=t_d2.word_counts['<UNK>']+counts\n",
        "    \n",
        "  else:\n",
        "    t_d2.word_index[word]=t_d.word_index[word]+2\n",
        "    t_d2.word_counts[word]=t_d.word_counts[word]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "srFkppQvxYcN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#<s>=1\n",
        "#</s>=2\n",
        "#<pad>=0\n",
        "#UNK=3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PrgsS5aSx0wD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "t2.word_index['<pad>']=0\n",
        "t2.word_index['<start>']=1\n",
        "t2.word_index['<end>']=2\n",
        "\n",
        "t_d2.word_index['<pad>']=0\n",
        "t_d2.word_index['<start>']=1\n",
        "t_d2.word_index['<end>']=2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fm9NXmur9Ia8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###debugging....nothing to see here..move on.."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "d668af82-cf18-4641-f2c4-7526fdb7ecdc",
        "id": "2gyVBTMr9Fdl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "a=\"I'm a gOat wiTh a giant oo'pp!djfkd.\"\n",
        "a=keras.preprocessing.text.text_to_word_sequence(a)\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "[w.translate(table) for w in a]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['im', 'a', 'goat', 'with', 'a', 'giant', 'oopp', 'djfkd']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "UTUwtL8lUsLP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "117e1480-7d51-438b-8764-e44016498908"
      },
      "cell_type": "code",
      "source": [
        "a=\"I'm a gOat wiTh a giant oo'pp!djfkd.\"\n",
        "a=a.split(' ')\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "a=[w.translate(table).lower() for w in a]\n",
        "a"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['im', 'a', 'goat', 'with', 'a', 'giant', 'ooppdjfkd']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "o9e_mfdW9Zpz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Functions to get padded, indexed sequence"
      ]
    },
    {
      "metadata": {
        "id": "HKEInOdSxNpt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def index_batch(t,batch,flag): #batch will be a list of size [batchsize] with each item being a sentence\n",
        "  for i in range(len(batch)):\n",
        "    batch[i]=batch[i].split(' ')\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    stripped = [w.translate(table).lower() for w in batch[i]]\n",
        "    for j in range(len(batch[i])):\n",
        "      if stripped[j]  not in t.word_index:\n",
        "        batch[i][j]=3\n",
        "      else:\n",
        "        batch[i][j]=t.word_index[stripped[j].lower()]\n",
        "    if flag=='decoder':\n",
        "      start_token=np.array(1)\n",
        "      stop_token=np.array(2)\n",
        "      batch[i]=np.hstack([start_token,batch[i],stop_token])\n",
        "\n",
        "  return batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C2A2BMX87OVd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_batch(text,t,flag='encoder'):  #returns the padded indexed np.array of given batch number\n",
        "  batch=index_batch(t,text,flag)\n",
        "  batch=keras.preprocessing.sequence.pad_sequences(batch,dtype=int,padding='post',truncating='post',value=0)\n",
        "  return batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1ZXa0NQQze1o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Parameters"
      ]
    },
    {
      "metadata": {
        "id": "v5OjoZ50xv8l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "global batch_size, data_size,embedding_dims,units\n",
        "learning_rate = 0.001\n",
        "BUFFER_SIZE = len(en_m)\n",
        "batch_size=32\n",
        "N_BATCH = BUFFER_SIZE//batch_size\n",
        "units=128\n",
        "data_size = len(en_m)\n",
        "en_vocab_size=len(t2.word_counts)+3 #same as below\n",
        "de_vocab_size=len(t_d2.word_counts)+3 #adding start stop and pad tokens\n",
        "embedding_dims=50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2m2dzxjF96vx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Create dataset"
      ]
    },
    {
      "metadata": {
        "id": "PI9Z_cXFzPIm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ucbbW56d5fJe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MyData(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.data = X\n",
        "        self.target = y\n",
        "        # TODO: convert this into torch code is possible\n",
        "        self.length = [ np.sum(1 - np.equal(x, 0)) for x in X]\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "        x_len = self.length[index]\n",
        "        return x,y,x_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KOztHMTW53m6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_tensor = get_batch(en_m,t2,'decoder')\n",
        "target_tensor = get_batch(de_m,t_d2,'decoder')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y_yIQvCY5i8i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dataset = MyData(input_tensor, target_tensor)\n",
        "dataset = DataLoader(train_dataset, batch_size, \n",
        "                      drop_last=True,\n",
        "                      shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zIZJSX7g-Bj5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Encoder class"
      ]
    },
    {
      "metadata": {
        "id": "eTqDKfiI6RM1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.gru = nn.LSTM(self.embedding_dim, self.enc_units)\n",
        "        \n",
        "    def forward(self, x, lens, device):\n",
        "        # x: batch_size, max_length \n",
        "        \n",
        "        # x: batch_size, max_length, embedding_dim\n",
        "        x=x.transpose(0,1)\n",
        "        x = self.embedding(x) \n",
        "        #print(x.shape)        \n",
        "        # x transformed = max_len X batch_size X embedding_dim\n",
        "        x = x.permute(1,0,2)\n",
        "        #print(x.shape)\n",
        "        x = pack_padded_sequence(x, lens) # unpad\n",
        "    \n",
        "        self.hidden_state = self.initialize_hidden_state(device)\n",
        "        self.cell_state = self.initialize_hidden_state(device)\n",
        "        \n",
        "        # output: max_length, batch_size, enc_units\n",
        "        # self.hidden: 1, batch_size, enc_units\n",
        "        output, (self.hidden_state,self.cell_state) = self.gru(x, (self.hidden_state,self.cell_state)) # gru returns hidden state of all timesteps as well as hidden state at last timestep\n",
        "        \n",
        "        # pad the sequence to the max length in the batch\n",
        "        output, _ = pad_packed_sequence(output)\n",
        "        \n",
        "        return output, (self.hidden_state, self.cell_state)\n",
        "\n",
        "    def initialize_hidden_state(self, device):\n",
        "        return torch.zeros((1, self.batch_sz, self.enc_units)).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3gRe7-TA6Szv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### sort batch function to be able to use with pad_packed_sequence\n",
        "def sort_batch(X, y, lengths):\n",
        "    lengths, indx = lengths.sort(dim=0, descending=True)\n",
        "    X = X[indx]\n",
        "    y = y[indx]\n",
        "    return X.transpose(0,1), y, lengths # transpose (batch x seq) to (seq x batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YUBeQzrwG7zI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c02047f1-2098-4466-8505-fb604cdab696"
      },
      "cell_type": "code",
      "source": [
        "### Testing Encoder part\n",
        "# TODO: put whether GPU is available or not\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device = torch.device(\"cpu\")\n",
        "encoder = Encoder(en_vocab_size, embedding_dims, units, batch_size)\n",
        "\n",
        "encoder.to(device)\n",
        "# obtain one sample from the data iterator\n",
        "it = iter(dataset)\n",
        "x, y, x_len = next(it)\n",
        "\n",
        "# sort the batch first to be able to use with pac_pack_sequence\n",
        "xs, ys, lens = sort_batch(x, y, x_len)\n",
        "\n",
        "enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
        "\n",
        "print(enc_output.size()) # max_length, batch_size, enc_units"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([17, 32, 128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QDrGIqsM-Mnz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Decoder class"
      ]
    },
    {
      "metadata": {
        "id": "nLTGgSRKJUcS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, enc_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.enc_units = enc_units\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.gru = nn.LSTM(self.embedding_dim + self.enc_units, \n",
        "                          self.dec_units,\n",
        "                          batch_first=True)\n",
        "        self.fc = nn.Linear(self.enc_units, self.vocab_size)\n",
        "        \n",
        "        # used for attention\n",
        "        self.W1 = nn.Linear(self.enc_units, self.dec_units)\n",
        "        self.W2 = nn.Linear(self.enc_units, self.dec_units)\n",
        "        self.V = nn.Linear(self.enc_units, 1)\n",
        "    \n",
        "    def forward(self, x, hidden,cell, enc_output):\n",
        "        # enc_output original: (max_length, batch_size, enc_units)\n",
        "        # enc_output converted == (batch_size, max_length, hidden_size)\n",
        "        enc_output = enc_output.permute(1,0,2)\n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        \n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        hidden_with_time_axis = hidden.permute(1, 0, 2)\n",
        "        \n",
        "        # score: (batch_size, max_length, hidden_size) # Bahdanaus's\n",
        "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
        "        # It doesn't matter which FC we pick for each of the inputs\n",
        "        score = torch.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n",
        "        \n",
        "        #score = torch.tanh(self.W2(hidden_with_time_axis) + self.W1(enc_output))\n",
        "          \n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        attention_weights = torch.softmax(self.V(score), dim=1)\n",
        "        \n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = torch.sum(context_vector, dim=1)\n",
        "        \n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        # takes case of the right portion of the model above (illustrated in red)\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        #x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        # ? Looks like attention vector in diagram of source\n",
        "        x = torch.cat((context_vector.unsqueeze(1), x), -1)\n",
        "        \n",
        "        # passing the concatenated vector to the GRU\n",
        "        # output: (batch_size, 1, hidden_size)\n",
        "        output, (hstate,cstate) = self.gru(x)\n",
        "        \n",
        "        \n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output =  output.view(-1, output.size(2))\n",
        "        \n",
        "        # output shape == (batch_size * 1, vocab)\n",
        "        x = self.fc(output)\n",
        "        \n",
        "        return x, (hstate,cstate) ,attention_weights\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        return torch.zeros((2,1, self.batch_sz, self.dec_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_2zTyFWyJpaN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "19975f95-d888-4d65-a8e7-372f4b8e1da8"
      },
      "cell_type": "code",
      "source": [
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder = Encoder(en_vocab_size, embedding_dims, units, batch_size)\n",
        "\n",
        "encoder.to(device)\n",
        "# obtain one sample from the data iterator\n",
        "it = iter(dataset)\n",
        "x, y, x_len = next(it)\n",
        "\n",
        "print(\"Input: \", x.shape)\n",
        "print(\"Output: \", y.shape)\n",
        "\n",
        "# sort the batch first to be able to use with pac_pack_sequence\n",
        "xs, ys, lens = sort_batch(x, y, x_len)\n",
        "\n",
        "enc_output, (enc_hidden,enc_cell) = encoder(xs.to(device), lens, device)\n",
        "print(\"Encoder Output: \", enc_output.shape) # batch_size X max_length X enc_units\n",
        "print(\"Encoder Hidden: \", enc_hidden[0].shape) # batch_size X enc_units (corresponds to the last state)\n",
        "\n",
        "decoder = Decoder(de_vocab_size, embedding_dims, units, units, batch_size)\n",
        "decoder = decoder.to(device)\n",
        "\n",
        "#print(enc_hidden.squeeze(0).shape)\n",
        "\n",
        "dec_hidden = enc_hidden#.squeeze(0)\n",
        "dec_cell = enc_cell\n",
        "dec_input = torch.tensor([[t_d2.word_index['<start>']]] * batch_size)\n",
        "print(\"Decoder Input: \", dec_input.shape)\n",
        "print(\"--------\")\n",
        "\n",
        "for t in range(1, y.size(1)):\n",
        "    # enc_hidden: 1, batch_size, enc_units\n",
        "    # output: max_length, batch_size, enc_units\n",
        "    predictions, (dec_hidden, dec_cell), _ = decoder(dec_input.to(device), \n",
        "                                         dec_hidden.to(device),\n",
        "                                         dec_cell.to(device),\n",
        "                                         enc_output.to(device))\n",
        "    \n",
        "    print(\"Prediction: \", predictions.shape)\n",
        "    print(\"Decoder Hidden: \", dec_hidden.shape)\n",
        "    \n",
        "    #loss += loss_function(y[:, t].to(device), predictions.to(device))\n",
        "    \n",
        "    dec_input = y[:, t].unsqueeze(1)\n",
        "    print(dec_input.shape)\n",
        "    break"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:  torch.Size([32, 17])\n",
            "Output:  torch.Size([32, 17])\n",
            "Encoder Output:  torch.Size([17, 32, 128])\n",
            "Encoder Hidden:  torch.Size([32, 128])\n",
            "Decoder Input:  torch.Size([32, 1])\n",
            "--------\n",
            "Prediction:  torch.Size([32, 6993])\n",
            "Decoder Hidden:  torch.Size([1, 32, 128])\n",
            "torch.Size([32, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n0HSoLbn-UyZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Define loss function"
      ]
    },
    {
      "metadata": {
        "id": "XkU1cJScLcRL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    \"\"\" Only consider non-zero inputs in the loss; mask needed \"\"\"\n",
        "    #mask = 1 - np.equal(real, 0) # assign 0 to all above 0 and 1 to all 0s\n",
        "    #print(mask)\n",
        "    #print(real)  #[batch_size]\n",
        "    #print(pred.shape)  #[batch_size x targ_vocab_size]\n",
        "    #mask = real.ge(1).type(torch.cuda.FloatTensor)\n",
        "#    print(mask)\n",
        "    loss_ = criterion(pred, real) \n",
        "    return torch.mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KSytHedxWMZG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = torch.device(\"cpu\")\n",
        "\n",
        "## TODO: Combine the encoder and decoder into one class\n",
        "encoder = Encoder(en_vocab_size, embedding_dims, units, batch_size)\n",
        "decoder = Decoder(de_vocab_size, embedding_dims, units, units, batch_size)\n",
        "\n",
        "encoder.to(device)\n",
        "decoder.to(device)\n",
        "\n",
        "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), \n",
        "                       lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hePv6fHE-Y-8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Train"
      ]
    },
    {
      "metadata": {
        "id": "pwqtiGvqWZ-o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9673
        },
        "outputId": "c721cfa4-b85c-4bb9-839f-06c834c9f813"
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    \n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, (inp, targ, inp_len)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "        \n",
        "        xs, ys, lens = sort_batch(inp, targ, inp_len)\n",
        "        \n",
        "#         print(\"Output: \", targ.shape)\n",
        "        enc_output, (enc_hidden,enc_cell) = encoder(xs.to(device), lens, device)\n",
        "        dec_hidden = enc_hidden\n",
        "        dec_cell = enc_cell\n",
        "        \n",
        "        # use teacher forcing - feeding the target as the next input (via dec_input)\n",
        "        dec_input = torch.tensor([[t_d2.word_index['<start>']]] * batch_size)\n",
        "        \n",
        "        # run code below for every timestep in the ys batch\n",
        "        for t in range(1, ys.size(1)):\n",
        "            predictions, (dec_hidden,dec_cell), _ = decoder(dec_input.to(device),\n",
        "                                                            dec_hidden.to(device),\n",
        "                                                            dec_cell.to(device),\n",
        "                                                            enc_output.to(device))\n",
        "            \n",
        "            loss += loss_function(ys[:, t].to(device), predictions.to(device))\n",
        "            #loss += loss_\n",
        "            dec_input = ys[:, t].unsqueeze(1)\n",
        "            \n",
        "        \n",
        "        batch_loss = (loss / int(ys.size(1)))\n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        loss.backward()\n",
        "\n",
        "        ### UPDATE MODEL PARAMETERS\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.detach().item()))\n",
        "        \n",
        "        \n",
        "    ### TODO: Save checkpoint for model\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "    \n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 7 Batch 8000 Loss 1.6879\n",
            "Epoch 7 Batch 8100 Loss 1.2386\n",
            "Epoch 7 Batch 8200 Loss 1.4558\n",
            "Epoch 7 Batch 8300 Loss 1.5173\n",
            "Epoch 7 Batch 8400 Loss 1.9263\n",
            "Epoch 7 Batch 8500 Loss 1.5180\n",
            "Epoch 7 Batch 8600 Loss 1.6172\n",
            "Epoch 7 Batch 8700 Loss 1.5173\n",
            "Epoch 7 Batch 8800 Loss 1.3046\n",
            "Epoch 7 Batch 8900 Loss 1.6165\n",
            "Epoch 7 Batch 9000 Loss 1.6111\n",
            "Epoch 7 Batch 9100 Loss 1.4874\n",
            "Epoch 7 Batch 9200 Loss 1.5438\n",
            "Epoch 7 Batch 9300 Loss 1.9152\n",
            "Epoch 7 Batch 9400 Loss 1.5763\n",
            "Epoch 7 Batch 9500 Loss 1.7403\n",
            "Epoch 7 Batch 9600 Loss 1.2554\n",
            "Epoch 7 Batch 9700 Loss 1.5551\n",
            "Epoch 7 Batch 9800 Loss 1.5374\n",
            "Epoch 7 Batch 9900 Loss 1.5993\n",
            "Epoch 7 Batch 10000 Loss 1.4041\n",
            "Epoch 7 Batch 10100 Loss 1.5851\n",
            "Epoch 7 Batch 10200 Loss 1.3408\n",
            "Epoch 7 Batch 10300 Loss 1.7249\n",
            "Epoch 7 Batch 10400 Loss 1.5104\n",
            "Epoch 7 Batch 10500 Loss 1.5593\n",
            "Epoch 7 Batch 10600 Loss 1.3964\n",
            "Epoch 7 Batch 10700 Loss 1.5700\n",
            "Epoch 7 Batch 10800 Loss 1.5825\n",
            "Epoch 7 Batch 10900 Loss 1.3377\n",
            "Epoch 7 Batch 11000 Loss 1.6641\n",
            "Epoch 7 Batch 11100 Loss 1.4845\n",
            "Epoch 7 Batch 11200 Loss 1.6895\n",
            "Epoch 7 Batch 11300 Loss 1.2979\n",
            "Epoch 7 Batch 11400 Loss 1.5500\n",
            "Epoch 7 Batch 11500 Loss 1.8803\n",
            "Epoch 7 Batch 11600 Loss 1.8199\n",
            "Epoch 7 Batch 11700 Loss 1.2538\n",
            "Epoch 7 Batch 11800 Loss 1.3738\n",
            "Epoch 7 Batch 11900 Loss 1.7859\n",
            "Epoch 7 Batch 12000 Loss 1.4762\n",
            "Epoch 7 Batch 12100 Loss 1.5817\n",
            "Epoch 7 Batch 12200 Loss 1.5374\n",
            "Epoch 7 Batch 12300 Loss 1.3525\n",
            "Epoch 7 Batch 12400 Loss 1.3742\n",
            "Epoch 7 Batch 12500 Loss 1.5650\n",
            "Epoch 7 Batch 12600 Loss 1.4746\n",
            "Epoch 7 Batch 12700 Loss 1.4627\n",
            "Epoch 7 Batch 12800 Loss 1.5288\n",
            "Epoch 7 Batch 12900 Loss 1.6060\n",
            "Epoch 7 Batch 13000 Loss 1.7126\n",
            "Epoch 7 Batch 13100 Loss 1.5882\n",
            "Epoch 7 Batch 13200 Loss 1.4009\n",
            "Epoch 7 Batch 13300 Loss 1.3970\n",
            "Epoch 7 Batch 13400 Loss 1.5038\n",
            "Epoch 7 Batch 13500 Loss 1.3871\n",
            "Epoch 7 Batch 13600 Loss 1.5855\n",
            "Epoch 7 Batch 13700 Loss 1.5305\n",
            "Epoch 7 Batch 13800 Loss 1.5944\n",
            "Epoch 7 Batch 13900 Loss 1.3758\n",
            "Epoch 7 Batch 14000 Loss 1.1617\n",
            "Epoch 7 Batch 14100 Loss 1.5002\n",
            "Epoch 7 Batch 14200 Loss 1.3902\n",
            "Epoch 7 Batch 14300 Loss 1.3196\n",
            "Epoch 7 Batch 14400 Loss 1.4667\n",
            "Epoch 7 Batch 14500 Loss 1.4039\n",
            "Epoch 7 Batch 14600 Loss 1.3022\n",
            "Epoch 7 Batch 14700 Loss 1.7584\n",
            "Epoch 7 Batch 14800 Loss 1.2010\n",
            "Epoch 7 Batch 14900 Loss 1.4273\n",
            "Epoch 7 Batch 15000 Loss 2.0181\n",
            "Epoch 7 Batch 15100 Loss 1.4782\n",
            "Epoch 7 Batch 15200 Loss 1.4816\n",
            "Epoch 7 Batch 15300 Loss 1.4561\n",
            "Epoch 7 Batch 15400 Loss 1.8293\n",
            "Epoch 7 Batch 15500 Loss 1.5915\n",
            "Epoch 7 Batch 15600 Loss 1.7041\n",
            "Epoch 7 Batch 15700 Loss 1.8140\n",
            "Epoch 7 Batch 15800 Loss 1.6286\n",
            "Epoch 7 Loss 1.4977\n",
            "Time taken for 1 epoch 1346.3070447444916 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.8738\n",
            "Epoch 8 Batch 100 Loss 1.3461\n",
            "Epoch 8 Batch 200 Loss 1.2658\n",
            "Epoch 8 Batch 300 Loss 1.6852\n",
            "Epoch 8 Batch 400 Loss 1.5211\n",
            "Epoch 8 Batch 500 Loss 1.2247\n",
            "Epoch 8 Batch 600 Loss 1.9036\n",
            "Epoch 8 Batch 700 Loss 1.5720\n",
            "Epoch 8 Batch 800 Loss 1.6307\n",
            "Epoch 8 Batch 900 Loss 1.5561\n",
            "Epoch 8 Batch 1000 Loss 1.6020\n",
            "Epoch 8 Batch 1100 Loss 1.6311\n",
            "Epoch 8 Batch 1200 Loss 1.7660\n",
            "Epoch 8 Batch 1300 Loss 1.5529\n",
            "Epoch 8 Batch 1400 Loss 1.2965\n",
            "Epoch 8 Batch 1500 Loss 1.4468\n",
            "Epoch 8 Batch 1600 Loss 1.4052\n",
            "Epoch 8 Batch 1700 Loss 1.4516\n",
            "Epoch 8 Batch 1800 Loss 1.4591\n",
            "Epoch 8 Batch 1900 Loss 1.5610\n",
            "Epoch 8 Batch 2000 Loss 1.6277\n",
            "Epoch 8 Batch 2100 Loss 1.7710\n",
            "Epoch 8 Batch 2200 Loss 1.2714\n",
            "Epoch 8 Batch 2300 Loss 1.5174\n",
            "Epoch 8 Batch 2400 Loss 1.4421\n",
            "Epoch 8 Batch 2500 Loss 1.7406\n",
            "Epoch 8 Batch 2600 Loss 1.1645\n",
            "Epoch 8 Batch 2700 Loss 1.0592\n",
            "Epoch 8 Batch 2800 Loss 1.7359\n",
            "Epoch 8 Batch 2900 Loss 1.5360\n",
            "Epoch 8 Batch 3000 Loss 1.5332\n",
            "Epoch 8 Batch 3100 Loss 1.2958\n",
            "Epoch 8 Batch 3200 Loss 1.4280\n",
            "Epoch 8 Batch 3300 Loss 1.3900\n",
            "Epoch 8 Batch 3400 Loss 1.4032\n",
            "Epoch 8 Batch 3500 Loss 1.3721\n",
            "Epoch 8 Batch 3600 Loss 1.3231\n",
            "Epoch 8 Batch 3700 Loss 1.4306\n",
            "Epoch 8 Batch 3800 Loss 1.3199\n",
            "Epoch 8 Batch 3900 Loss 1.6256\n",
            "Epoch 8 Batch 4000 Loss 1.5304\n",
            "Epoch 8 Batch 4100 Loss 1.3035\n",
            "Epoch 8 Batch 4200 Loss 1.4632\n",
            "Epoch 8 Batch 4300 Loss 1.2796\n",
            "Epoch 8 Batch 4400 Loss 1.5636\n",
            "Epoch 8 Batch 4500 Loss 1.6289\n",
            "Epoch 8 Batch 4600 Loss 1.2583\n",
            "Epoch 8 Batch 4700 Loss 1.2938\n",
            "Epoch 8 Batch 4800 Loss 1.6543\n",
            "Epoch 8 Batch 4900 Loss 1.6994\n",
            "Epoch 8 Batch 5000 Loss 1.2800\n",
            "Epoch 8 Batch 5100 Loss 1.4727\n",
            "Epoch 8 Batch 5200 Loss 1.3867\n",
            "Epoch 8 Batch 5300 Loss 1.8239\n",
            "Epoch 8 Batch 5400 Loss 1.5138\n",
            "Epoch 8 Batch 5500 Loss 1.2916\n",
            "Epoch 8 Batch 5600 Loss 1.5477\n",
            "Epoch 8 Batch 5700 Loss 1.5558\n",
            "Epoch 8 Batch 5800 Loss 1.6938\n",
            "Epoch 8 Batch 5900 Loss 1.8364\n",
            "Epoch 8 Batch 6000 Loss 1.2155\n",
            "Epoch 8 Batch 6100 Loss 1.4454\n",
            "Epoch 8 Batch 6200 Loss 1.6141\n",
            "Epoch 8 Batch 6300 Loss 1.2591\n",
            "Epoch 8 Batch 6400 Loss 1.5034\n",
            "Epoch 8 Batch 6500 Loss 1.6447\n",
            "Epoch 8 Batch 6600 Loss 1.3347\n",
            "Epoch 8 Batch 6700 Loss 1.7421\n",
            "Epoch 8 Batch 6800 Loss 1.5065\n",
            "Epoch 8 Batch 6900 Loss 1.2573\n",
            "Epoch 8 Batch 7000 Loss 1.7439\n",
            "Epoch 8 Batch 7100 Loss 1.1813\n",
            "Epoch 8 Batch 7200 Loss 1.3957\n",
            "Epoch 8 Batch 7300 Loss 1.3883\n",
            "Epoch 8 Batch 7400 Loss 1.6531\n",
            "Epoch 8 Batch 7500 Loss 1.5431\n",
            "Epoch 8 Batch 7600 Loss 1.7783\n",
            "Epoch 8 Batch 7700 Loss 1.3038\n",
            "Epoch 8 Batch 7800 Loss 1.5424\n",
            "Epoch 8 Batch 7900 Loss 1.4298\n",
            "Epoch 8 Batch 8000 Loss 1.5860\n",
            "Epoch 8 Batch 8100 Loss 1.5034\n",
            "Epoch 8 Batch 8200 Loss 1.7088\n",
            "Epoch 8 Batch 8300 Loss 1.3794\n",
            "Epoch 8 Batch 8400 Loss 1.5753\n",
            "Epoch 8 Batch 8500 Loss 1.4406\n",
            "Epoch 8 Batch 8600 Loss 1.4240\n",
            "Epoch 8 Batch 8700 Loss 1.4649\n",
            "Epoch 8 Batch 8800 Loss 1.6255\n",
            "Epoch 8 Batch 8900 Loss 1.3087\n",
            "Epoch 8 Batch 9000 Loss 1.2926\n",
            "Epoch 8 Batch 9100 Loss 1.7637\n",
            "Epoch 8 Batch 9200 Loss 1.4660\n",
            "Epoch 8 Batch 9300 Loss 1.2499\n",
            "Epoch 8 Batch 9400 Loss 1.5715\n",
            "Epoch 8 Batch 9500 Loss 1.4639\n",
            "Epoch 8 Batch 9600 Loss 1.7132\n",
            "Epoch 8 Batch 9700 Loss 1.5963\n",
            "Epoch 8 Batch 9800 Loss 1.7545\n",
            "Epoch 8 Batch 9900 Loss 1.5777\n",
            "Epoch 8 Batch 10000 Loss 1.4416\n",
            "Epoch 8 Batch 10100 Loss 1.7477\n",
            "Epoch 8 Batch 10200 Loss 1.3007\n",
            "Epoch 8 Batch 10300 Loss 1.1785\n",
            "Epoch 8 Batch 10400 Loss 1.2884\n",
            "Epoch 8 Batch 10500 Loss 1.5818\n",
            "Epoch 8 Batch 10600 Loss 1.4016\n",
            "Epoch 8 Batch 10700 Loss 1.7853\n",
            "Epoch 8 Batch 10800 Loss 1.8005\n",
            "Epoch 8 Batch 10900 Loss 1.3339\n",
            "Epoch 8 Batch 11000 Loss 1.6703\n",
            "Epoch 8 Batch 11100 Loss 1.7464\n",
            "Epoch 8 Batch 11200 Loss 1.7150\n",
            "Epoch 8 Batch 11300 Loss 1.4686\n",
            "Epoch 8 Batch 11400 Loss 1.5180\n",
            "Epoch 8 Batch 11500 Loss 1.7890\n",
            "Epoch 8 Batch 11600 Loss 1.5044\n",
            "Epoch 8 Batch 11700 Loss 1.7022\n",
            "Epoch 8 Batch 11800 Loss 1.3317\n",
            "Epoch 8 Batch 11900 Loss 1.6527\n",
            "Epoch 8 Batch 12000 Loss 1.8185\n",
            "Epoch 8 Batch 12100 Loss 1.4575\n",
            "Epoch 8 Batch 12200 Loss 1.2523\n",
            "Epoch 8 Batch 12300 Loss 1.1689\n",
            "Epoch 8 Batch 12400 Loss 1.7201\n",
            "Epoch 8 Batch 12500 Loss 1.3658\n",
            "Epoch 8 Batch 12600 Loss 1.5182\n",
            "Epoch 8 Batch 12700 Loss 1.7607\n",
            "Epoch 8 Batch 12800 Loss 1.5635\n",
            "Epoch 8 Batch 12900 Loss 1.6639\n",
            "Epoch 8 Batch 13000 Loss 1.5005\n",
            "Epoch 8 Batch 13100 Loss 1.3256\n",
            "Epoch 8 Batch 13200 Loss 1.5977\n",
            "Epoch 8 Batch 13300 Loss 1.8096\n",
            "Epoch 8 Batch 13400 Loss 0.9148\n",
            "Epoch 8 Batch 13500 Loss 1.3813\n",
            "Epoch 8 Batch 13600 Loss 1.5815\n",
            "Epoch 8 Batch 13700 Loss 1.4403\n",
            "Epoch 8 Batch 13800 Loss 1.4641\n",
            "Epoch 8 Batch 13900 Loss 1.2969\n",
            "Epoch 8 Batch 14000 Loss 1.6360\n",
            "Epoch 8 Batch 14100 Loss 1.5009\n",
            "Epoch 8 Batch 14200 Loss 1.4840\n",
            "Epoch 8 Batch 14300 Loss 1.3959\n",
            "Epoch 8 Batch 14400 Loss 1.2366\n",
            "Epoch 8 Batch 14500 Loss 1.3576\n",
            "Epoch 8 Batch 14600 Loss 1.5439\n",
            "Epoch 8 Batch 14700 Loss 1.5741\n",
            "Epoch 8 Batch 14800 Loss 1.1784\n",
            "Epoch 8 Batch 14900 Loss 1.0492\n",
            "Epoch 8 Batch 15000 Loss 1.2825\n",
            "Epoch 8 Batch 15100 Loss 1.5565\n",
            "Epoch 8 Batch 15200 Loss 1.4945\n",
            "Epoch 8 Batch 15300 Loss 2.0293\n",
            "Epoch 8 Batch 15400 Loss 1.3570\n",
            "Epoch 8 Batch 15500 Loss 1.5778\n",
            "Epoch 8 Batch 15600 Loss 1.5100\n",
            "Epoch 8 Batch 15700 Loss 1.6797\n",
            "Epoch 8 Batch 15800 Loss 1.2584\n",
            "Epoch 8 Loss 1.4741\n",
            "Time taken for 1 epoch 1349.0878944396973 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.5707\n",
            "Epoch 9 Batch 100 Loss 1.2559\n",
            "Epoch 9 Batch 200 Loss 1.5777\n",
            "Epoch 9 Batch 300 Loss 1.7454\n",
            "Epoch 9 Batch 400 Loss 1.4290\n",
            "Epoch 9 Batch 500 Loss 1.4172\n",
            "Epoch 9 Batch 600 Loss 1.3867\n",
            "Epoch 9 Batch 700 Loss 1.3986\n",
            "Epoch 9 Batch 800 Loss 1.2117\n",
            "Epoch 9 Batch 900 Loss 1.1805\n",
            "Epoch 9 Batch 1000 Loss 1.4411\n",
            "Epoch 9 Batch 1100 Loss 1.5193\n",
            "Epoch 9 Batch 1200 Loss 1.0996\n",
            "Epoch 9 Batch 1300 Loss 1.5811\n",
            "Epoch 9 Batch 1400 Loss 1.3746\n",
            "Epoch 9 Batch 1500 Loss 1.3541\n",
            "Epoch 9 Batch 1600 Loss 1.0198\n",
            "Epoch 9 Batch 1700 Loss 1.6153\n",
            "Epoch 9 Batch 1800 Loss 1.5446\n",
            "Epoch 9 Batch 1900 Loss 1.5101\n",
            "Epoch 9 Batch 2000 Loss 1.5645\n",
            "Epoch 9 Batch 2100 Loss 1.6302\n",
            "Epoch 9 Batch 2200 Loss 1.3473\n",
            "Epoch 9 Batch 2300 Loss 1.0663\n",
            "Epoch 9 Batch 2400 Loss 1.3759\n",
            "Epoch 9 Batch 2500 Loss 1.6098\n",
            "Epoch 9 Batch 2600 Loss 1.4976\n",
            "Epoch 9 Batch 2700 Loss 1.3924\n",
            "Epoch 9 Batch 2800 Loss 1.0980\n",
            "Epoch 9 Batch 2900 Loss 1.6066\n",
            "Epoch 9 Batch 3000 Loss 1.4284\n",
            "Epoch 9 Batch 3100 Loss 1.4613\n",
            "Epoch 9 Batch 3200 Loss 1.3788\n",
            "Epoch 9 Batch 3300 Loss 1.4754\n",
            "Epoch 9 Batch 3400 Loss 1.5977\n",
            "Epoch 9 Batch 3500 Loss 1.6397\n",
            "Epoch 9 Batch 3600 Loss 1.4328\n",
            "Epoch 9 Batch 3700 Loss 1.3036\n",
            "Epoch 9 Batch 3800 Loss 1.3384\n",
            "Epoch 9 Batch 3900 Loss 1.4603\n",
            "Epoch 9 Batch 4000 Loss 1.4531\n",
            "Epoch 9 Batch 4100 Loss 1.2173\n",
            "Epoch 9 Batch 4200 Loss 1.5733\n",
            "Epoch 9 Batch 4300 Loss 1.1877\n",
            "Epoch 9 Batch 4400 Loss 1.5881\n",
            "Epoch 9 Batch 4500 Loss 1.4376\n",
            "Epoch 9 Batch 4600 Loss 1.2456\n",
            "Epoch 9 Batch 4700 Loss 1.4267\n",
            "Epoch 9 Batch 4800 Loss 1.2876\n",
            "Epoch 9 Batch 4900 Loss 1.2702\n",
            "Epoch 9 Batch 5000 Loss 1.7608\n",
            "Epoch 9 Batch 5100 Loss 1.3683\n",
            "Epoch 9 Batch 5200 Loss 1.2732\n",
            "Epoch 9 Batch 5300 Loss 1.3362\n",
            "Epoch 9 Batch 5400 Loss 1.7098\n",
            "Epoch 9 Batch 5500 Loss 1.1616\n",
            "Epoch 9 Batch 5600 Loss 1.7413\n",
            "Epoch 9 Batch 5700 Loss 1.6628\n",
            "Epoch 9 Batch 5800 Loss 1.2125\n",
            "Epoch 9 Batch 5900 Loss 1.1850\n",
            "Epoch 9 Batch 6000 Loss 1.5427\n",
            "Epoch 9 Batch 6100 Loss 1.4068\n",
            "Epoch 9 Batch 6200 Loss 1.6569\n",
            "Epoch 9 Batch 6300 Loss 1.0447\n",
            "Epoch 9 Batch 6400 Loss 1.6235\n",
            "Epoch 9 Batch 6500 Loss 1.7143\n",
            "Epoch 9 Batch 6600 Loss 1.6319\n",
            "Epoch 9 Batch 6700 Loss 1.4151\n",
            "Epoch 9 Batch 6800 Loss 1.8728\n",
            "Epoch 9 Batch 6900 Loss 1.3493\n",
            "Epoch 9 Batch 7000 Loss 1.6703\n",
            "Epoch 9 Batch 7100 Loss 1.3762\n",
            "Epoch 9 Batch 7200 Loss 1.2994\n",
            "Epoch 9 Batch 7300 Loss 1.7371\n",
            "Epoch 9 Batch 7400 Loss 1.1958\n",
            "Epoch 9 Batch 7500 Loss 1.1140\n",
            "Epoch 9 Batch 7600 Loss 1.2600\n",
            "Epoch 9 Batch 7700 Loss 1.1039\n",
            "Epoch 9 Batch 7800 Loss 1.2701\n",
            "Epoch 9 Batch 7900 Loss 1.1411\n",
            "Epoch 9 Batch 8000 Loss 1.3835\n",
            "Epoch 9 Batch 8100 Loss 1.5110\n",
            "Epoch 9 Batch 8200 Loss 1.2672\n",
            "Epoch 9 Batch 8300 Loss 1.5983\n",
            "Epoch 9 Batch 8400 Loss 1.7101\n",
            "Epoch 9 Batch 8500 Loss 1.3669\n",
            "Epoch 9 Batch 8600 Loss 1.6561\n",
            "Epoch 9 Batch 8700 Loss 1.4553\n",
            "Epoch 9 Batch 8800 Loss 1.4708\n",
            "Epoch 9 Batch 8900 Loss 1.5039\n",
            "Epoch 9 Batch 9000 Loss 1.5960\n",
            "Epoch 9 Batch 9100 Loss 1.6362\n",
            "Epoch 9 Batch 9200 Loss 1.5460\n",
            "Epoch 9 Batch 9300 Loss 1.5488\n",
            "Epoch 9 Batch 9400 Loss 1.2747\n",
            "Epoch 9 Batch 9500 Loss 1.5078\n",
            "Epoch 9 Batch 9600 Loss 1.9196\n",
            "Epoch 9 Batch 9700 Loss 1.5132\n",
            "Epoch 9 Batch 9800 Loss 1.5602\n",
            "Epoch 9 Batch 9900 Loss 1.5450\n",
            "Epoch 9 Batch 10000 Loss 1.4409\n",
            "Epoch 9 Batch 10100 Loss 1.4772\n",
            "Epoch 9 Batch 10200 Loss 1.4957\n",
            "Epoch 9 Batch 10300 Loss 1.3067\n",
            "Epoch 9 Batch 10400 Loss 1.6467\n",
            "Epoch 9 Batch 10500 Loss 1.5974\n",
            "Epoch 9 Batch 10600 Loss 1.5358\n",
            "Epoch 9 Batch 10700 Loss 1.2852\n",
            "Epoch 9 Batch 10800 Loss 1.7189\n",
            "Epoch 9 Batch 10900 Loss 1.4377\n",
            "Epoch 9 Batch 11000 Loss 1.5607\n",
            "Epoch 9 Batch 11100 Loss 1.3326\n",
            "Epoch 9 Batch 11200 Loss 1.5581\n",
            "Epoch 9 Batch 11300 Loss 1.4141\n",
            "Epoch 9 Batch 11400 Loss 1.1935\n",
            "Epoch 9 Batch 11500 Loss 1.4429\n",
            "Epoch 9 Batch 11600 Loss 1.4769\n",
            "Epoch 9 Batch 11700 Loss 1.7255\n",
            "Epoch 9 Batch 11800 Loss 1.4280\n",
            "Epoch 9 Batch 11900 Loss 1.0771\n",
            "Epoch 9 Batch 12000 Loss 1.5888\n",
            "Epoch 9 Batch 12100 Loss 1.2129\n",
            "Epoch 9 Batch 12200 Loss 1.7106\n",
            "Epoch 9 Batch 12300 Loss 1.5312\n",
            "Epoch 9 Batch 12400 Loss 1.5633\n",
            "Epoch 9 Batch 12500 Loss 1.3407\n",
            "Epoch 9 Batch 12600 Loss 1.4432\n",
            "Epoch 9 Batch 12700 Loss 1.3137\n",
            "Epoch 9 Batch 12800 Loss 1.4580\n",
            "Epoch 9 Batch 12900 Loss 1.2636\n",
            "Epoch 9 Batch 13000 Loss 1.4774\n",
            "Epoch 9 Batch 13100 Loss 1.3637\n",
            "Epoch 9 Batch 13200 Loss 1.3920\n",
            "Epoch 9 Batch 13300 Loss 1.3067\n",
            "Epoch 9 Batch 13400 Loss 1.4355\n",
            "Epoch 9 Batch 13500 Loss 1.1525\n",
            "Epoch 9 Batch 13600 Loss 1.5522\n",
            "Epoch 9 Batch 13700 Loss 1.4093\n",
            "Epoch 9 Batch 13800 Loss 1.6515\n",
            "Epoch 9 Batch 13900 Loss 1.5989\n",
            "Epoch 9 Batch 14000 Loss 1.1848\n",
            "Epoch 9 Batch 14100 Loss 1.5920\n",
            "Epoch 9 Batch 14200 Loss 1.4314\n",
            "Epoch 9 Batch 14300 Loss 1.5415\n",
            "Epoch 9 Batch 14400 Loss 1.6030\n",
            "Epoch 9 Batch 14500 Loss 1.3991\n",
            "Epoch 9 Batch 14600 Loss 1.1730\n",
            "Epoch 9 Batch 14700 Loss 1.2344\n",
            "Epoch 9 Batch 14800 Loss 1.3793\n",
            "Epoch 9 Batch 14900 Loss 1.3932\n",
            "Epoch 9 Batch 15000 Loss 1.5480\n",
            "Epoch 9 Batch 15100 Loss 1.6448\n",
            "Epoch 9 Batch 15200 Loss 1.2312\n",
            "Epoch 9 Batch 15300 Loss 1.1127\n",
            "Epoch 9 Batch 15400 Loss 1.5929\n",
            "Epoch 9 Batch 15500 Loss 1.6267\n",
            "Epoch 9 Batch 15600 Loss 1.6789\n",
            "Epoch 9 Batch 15700 Loss 1.6716\n",
            "Epoch 9 Batch 15800 Loss 1.5347\n",
            "Epoch 9 Loss 1.4548\n",
            "Time taken for 1 epoch 1347.6437492370605 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.3340\n",
            "Epoch 10 Batch 100 Loss 1.5032\n",
            "Epoch 10 Batch 200 Loss 1.2185\n",
            "Epoch 10 Batch 300 Loss 1.4334\n",
            "Epoch 10 Batch 400 Loss 1.4059\n",
            "Epoch 10 Batch 500 Loss 1.1806\n",
            "Epoch 10 Batch 600 Loss 1.3445\n",
            "Epoch 10 Batch 700 Loss 1.4952\n",
            "Epoch 10 Batch 800 Loss 1.4951\n",
            "Epoch 10 Batch 900 Loss 1.7921\n",
            "Epoch 10 Batch 1000 Loss 1.1789\n",
            "Epoch 10 Batch 1100 Loss 1.5289\n",
            "Epoch 10 Batch 1200 Loss 1.2972\n",
            "Epoch 10 Batch 1300 Loss 1.8005\n",
            "Epoch 10 Batch 1400 Loss 1.3608\n",
            "Epoch 10 Batch 1500 Loss 1.4452\n",
            "Epoch 10 Batch 1600 Loss 1.3332\n",
            "Epoch 10 Batch 1700 Loss 1.5149\n",
            "Epoch 10 Batch 1800 Loss 1.3164\n",
            "Epoch 10 Batch 1900 Loss 1.3684\n",
            "Epoch 10 Batch 2000 Loss 1.3867\n",
            "Epoch 10 Batch 2100 Loss 1.6723\n",
            "Epoch 10 Batch 2200 Loss 1.5108\n",
            "Epoch 10 Batch 2300 Loss 1.2420\n",
            "Epoch 10 Batch 2400 Loss 1.5558\n",
            "Epoch 10 Batch 2500 Loss 1.2281\n",
            "Epoch 10 Batch 2600 Loss 1.3904\n",
            "Epoch 10 Batch 2700 Loss 1.3117\n",
            "Epoch 10 Batch 2800 Loss 1.4297\n",
            "Epoch 10 Batch 2900 Loss 1.2736\n",
            "Epoch 10 Batch 3000 Loss 1.4307\n",
            "Epoch 10 Batch 3100 Loss 1.6074\n",
            "Epoch 10 Batch 3200 Loss 1.4542\n",
            "Epoch 10 Batch 3300 Loss 1.7328\n",
            "Epoch 10 Batch 3400 Loss 1.4781\n",
            "Epoch 10 Batch 3500 Loss 1.2778\n",
            "Epoch 10 Batch 3600 Loss 1.2290\n",
            "Epoch 10 Batch 3700 Loss 1.2781\n",
            "Epoch 10 Batch 3800 Loss 1.3300\n",
            "Epoch 10 Batch 3900 Loss 1.1973\n",
            "Epoch 10 Batch 4000 Loss 1.5442\n",
            "Epoch 10 Batch 4100 Loss 1.2334\n",
            "Epoch 10 Batch 4200 Loss 1.4275\n",
            "Epoch 10 Batch 4300 Loss 1.4005\n",
            "Epoch 10 Batch 4400 Loss 1.4522\n",
            "Epoch 10 Batch 4500 Loss 1.9417\n",
            "Epoch 10 Batch 4600 Loss 1.3709\n",
            "Epoch 10 Batch 4700 Loss 1.5493\n",
            "Epoch 10 Batch 4800 Loss 1.3539\n",
            "Epoch 10 Batch 4900 Loss 1.2128\n",
            "Epoch 10 Batch 5000 Loss 1.4220\n",
            "Epoch 10 Batch 5100 Loss 1.4447\n",
            "Epoch 10 Batch 5200 Loss 1.3447\n",
            "Epoch 10 Batch 5300 Loss 1.2963\n",
            "Epoch 10 Batch 5400 Loss 1.3599\n",
            "Epoch 10 Batch 5500 Loss 1.3957\n",
            "Epoch 10 Batch 5600 Loss 1.3765\n",
            "Epoch 10 Batch 5700 Loss 1.0471\n",
            "Epoch 10 Batch 5800 Loss 1.3011\n",
            "Epoch 10 Batch 5900 Loss 1.3568\n",
            "Epoch 10 Batch 6000 Loss 1.3966\n",
            "Epoch 10 Batch 6100 Loss 1.5451\n",
            "Epoch 10 Batch 6200 Loss 1.8391\n",
            "Epoch 10 Batch 6300 Loss 1.7664\n",
            "Epoch 10 Batch 6400 Loss 1.2907\n",
            "Epoch 10 Batch 6500 Loss 1.5710\n",
            "Epoch 10 Batch 6600 Loss 1.1361\n",
            "Epoch 10 Batch 6700 Loss 1.5145\n",
            "Epoch 10 Batch 6800 Loss 1.4152\n",
            "Epoch 10 Batch 6900 Loss 1.3088\n",
            "Epoch 10 Batch 7000 Loss 1.3071\n",
            "Epoch 10 Batch 7100 Loss 1.6060\n",
            "Epoch 10 Batch 7200 Loss 1.2204\n",
            "Epoch 10 Batch 7300 Loss 1.3436\n",
            "Epoch 10 Batch 7400 Loss 1.2769\n",
            "Epoch 10 Batch 7500 Loss 1.3821\n",
            "Epoch 10 Batch 7600 Loss 1.6318\n",
            "Epoch 10 Batch 7700 Loss 1.3815\n",
            "Epoch 10 Batch 7800 Loss 1.3206\n",
            "Epoch 10 Batch 7900 Loss 1.0246\n",
            "Epoch 10 Batch 8000 Loss 1.3793\n",
            "Epoch 10 Batch 8100 Loss 1.5488\n",
            "Epoch 10 Batch 8200 Loss 1.4827\n",
            "Epoch 10 Batch 8300 Loss 1.2983\n",
            "Epoch 10 Batch 8400 Loss 1.3559\n",
            "Epoch 10 Batch 8500 Loss 1.6993\n",
            "Epoch 10 Batch 8600 Loss 1.5853\n",
            "Epoch 10 Batch 8700 Loss 1.0800\n",
            "Epoch 10 Batch 8800 Loss 1.1534\n",
            "Epoch 10 Batch 8900 Loss 1.5523\n",
            "Epoch 10 Batch 9000 Loss 1.5353\n",
            "Epoch 10 Batch 9100 Loss 1.4214\n",
            "Epoch 10 Batch 9200 Loss 1.5377\n",
            "Epoch 10 Batch 9300 Loss 1.6467\n",
            "Epoch 10 Batch 9400 Loss 1.2799\n",
            "Epoch 10 Batch 9500 Loss 1.3505\n",
            "Epoch 10 Batch 9600 Loss 1.7392\n",
            "Epoch 10 Batch 9700 Loss 1.3852\n",
            "Epoch 10 Batch 9800 Loss 1.3914\n",
            "Epoch 10 Batch 9900 Loss 1.2826\n",
            "Epoch 10 Batch 10000 Loss 1.5151\n",
            "Epoch 10 Batch 10100 Loss 1.3221\n",
            "Epoch 10 Batch 10200 Loss 1.6031\n",
            "Epoch 10 Batch 10300 Loss 1.2134\n",
            "Epoch 10 Batch 10400 Loss 1.3653\n",
            "Epoch 10 Batch 10500 Loss 1.5048\n",
            "Epoch 10 Batch 10600 Loss 1.7148\n",
            "Epoch 10 Batch 10700 Loss 1.3158\n",
            "Epoch 10 Batch 10800 Loss 1.4417\n",
            "Epoch 10 Batch 10900 Loss 1.5912\n",
            "Epoch 10 Batch 11000 Loss 1.4053\n",
            "Epoch 10 Batch 11100 Loss 1.4607\n",
            "Epoch 10 Batch 11200 Loss 1.2889\n",
            "Epoch 10 Batch 11300 Loss 1.5432\n",
            "Epoch 10 Batch 11400 Loss 1.5477\n",
            "Epoch 10 Batch 11500 Loss 1.4817\n",
            "Epoch 10 Batch 11600 Loss 1.3401\n",
            "Epoch 10 Batch 11700 Loss 1.1298\n",
            "Epoch 10 Batch 11800 Loss 1.7339\n",
            "Epoch 10 Batch 11900 Loss 1.2927\n",
            "Epoch 10 Batch 12000 Loss 1.4435\n",
            "Epoch 10 Batch 12100 Loss 1.3882\n",
            "Epoch 10 Batch 12200 Loss 1.6696\n",
            "Epoch 10 Batch 12300 Loss 1.4285\n",
            "Epoch 10 Batch 12400 Loss 1.6298\n",
            "Epoch 10 Batch 12500 Loss 1.5452\n",
            "Epoch 10 Batch 12600 Loss 1.4940\n",
            "Epoch 10 Batch 12700 Loss 1.4528\n",
            "Epoch 10 Batch 12800 Loss 1.6615\n",
            "Epoch 10 Batch 12900 Loss 1.3885\n",
            "Epoch 10 Batch 13000 Loss 1.5426\n",
            "Epoch 10 Batch 13100 Loss 1.4617\n",
            "Epoch 10 Batch 13200 Loss 1.4592\n",
            "Epoch 10 Batch 13300 Loss 1.5825\n",
            "Epoch 10 Batch 13400 Loss 1.6938\n",
            "Epoch 10 Batch 13500 Loss 1.3328\n",
            "Epoch 10 Batch 13600 Loss 1.3823\n",
            "Epoch 10 Batch 13700 Loss 1.4808\n",
            "Epoch 10 Batch 13800 Loss 1.6262\n",
            "Epoch 10 Batch 13900 Loss 1.5409\n",
            "Epoch 10 Batch 14000 Loss 1.1461\n",
            "Epoch 10 Batch 14100 Loss 1.2702\n",
            "Epoch 10 Batch 14200 Loss 1.5153\n",
            "Epoch 10 Batch 14300 Loss 1.3506\n",
            "Epoch 10 Batch 14400 Loss 1.6742\n",
            "Epoch 10 Batch 14500 Loss 1.2012\n",
            "Epoch 10 Batch 14600 Loss 1.5214\n",
            "Epoch 10 Batch 14700 Loss 1.4496\n",
            "Epoch 10 Batch 14800 Loss 1.3525\n",
            "Epoch 10 Batch 14900 Loss 1.3160\n",
            "Epoch 10 Batch 15000 Loss 1.3762\n",
            "Epoch 10 Batch 15100 Loss 1.6706\n",
            "Epoch 10 Batch 15200 Loss 1.3517\n",
            "Epoch 10 Batch 15300 Loss 1.4899\n",
            "Epoch 10 Batch 15400 Loss 1.4048\n",
            "Epoch 10 Batch 15500 Loss 1.1989\n",
            "Epoch 10 Batch 15600 Loss 1.5072\n",
            "Epoch 10 Batch 15700 Loss 1.7471\n",
            "Epoch 10 Batch 15800 Loss 1.5084\n",
            "Epoch 10 Loss 1.4384\n",
            "Time taken for 1 epoch 1367.1744375228882 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I5baz9IK6tJT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Saving learned parameters"
      ]
    },
    {
      "metadata": {
        "id": "bCfUkSsdwu7Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.save(encoder.state_dict(),'/content/drive/My Drive/nmt/correct_enc_additive_sent15_freq40')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_7jF1hr0wWcD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.save(decoder.state_dict(),'/content/drive/My Drive/nmt/correct_dec_additive_sent15_freq40')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sH-Qcr3L4e3H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "9abbb809-527a-4511-f1f0-8fd49b093935"
      },
      "cell_type": "code",
      "source": [
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in decoder.state_dict():\n",
        "    print(param_tensor, \"\\t\", decoder.state_dict()[param_tensor].size())\n"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model's state_dict:\n",
            "embedding.weight \t torch.Size([6993, 50])\n",
            "gru.weight_ih_l0 \t torch.Size([512, 178])\n",
            "gru.weight_hh_l0 \t torch.Size([512, 128])\n",
            "gru.bias_ih_l0 \t torch.Size([512])\n",
            "gru.bias_hh_l0 \t torch.Size([512])\n",
            "fc.weight \t torch.Size([6993, 128])\n",
            "fc.bias \t torch.Size([6993])\n",
            "W1.weight \t torch.Size([128, 128])\n",
            "W1.bias \t torch.Size([128])\n",
            "W2.weight \t torch.Size([128, 128])\n",
            "W2.bias \t torch.Size([128])\n",
            "V.weight \t torch.Size([1, 128])\n",
            "V.bias \t torch.Size([1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BxkrTRvT6mEV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Loading learned parameters"
      ]
    },
    {
      "metadata": {
        "id": "Y2wuWpZI07hD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder = Decoder(de_vocab_size, embedding_dims, units, units, batch_size)\n",
        "decoder = decoder.load_state_dict(torch.load('/content/drive/My Drive/nmt/correct_dec_additive_sent15_freq40'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R9JMJVHg6cr0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dec"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}